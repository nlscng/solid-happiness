{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with ground truth function:\n",
    "\n",
    "`f = 2 * x`\n",
    "\n",
    "And our model:\n",
    "\n",
    "`f = w * x`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we have these steps:\n",
    "\n",
    "1) Design model (input size, output size, forward pass) \n",
    "\n",
    "2) Construct loss and optimizer function\n",
    "\n",
    "3) Training loop:\n",
    "    - forward pass: compute predictions\n",
    "    - backward pass: calculate gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# import neural net module\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X and Y needs to comform to a specific shape to use model, \n",
    "# where row count is sample count, and column count is feature count\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_sample, n_feature = X.shape\n",
    "X.shape\n",
    "\n",
    "# Dont need explicit w anymore, we will let model take care of it.\n",
    "# w = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "model = nn.Linear(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are to wrap a built-in linear regression model\n",
    "class MyLinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MyLinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = MyLinearRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables, for now\n",
    "learning_rate = 0.03\n",
    "n_iters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old, manual model prediction\n",
    "# def forward(x):\n",
    "#     return w * x\n",
    "\n",
    "# loss = MSE\n",
    "# def loss(y, y_hat):\n",
    "#     return ((y - y_hat) ** 2).mean()\n",
    "\n",
    "loss = nn.MSELoss() # note this is a callable function\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test tensor for the before training\n",
    "X_test = torch.tensor([5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prediction before training: f(5) = 1.821'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a tensor with a single value can has its value unwrapped by calling item()\n",
    "f'Prediction before training: f(5) = {model(X_test).item():.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.135, loss = 20.51086235\n",
      "epoch 3: w = 1.693, loss = 1.29222107\n",
      "epoch 5: w = 1.834, loss = 0.10007751\n",
      "epoch 7: w = 1.870, loss = 0.02546586\n",
      "epoch 9: w = 1.881, loss = 0.02015778\n",
      "epoch 11: w = 1.885, loss = 0.01917017\n",
      "epoch 13: w = 1.888, loss = 0.01847376\n",
      "epoch 15: w = 1.890, loss = 0.01781785\n",
      "epoch 17: w = 1.892, loss = 0.01718624\n",
      "epoch 19: w = 1.894, loss = 0.01657704\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    # predict\n",
    "    y_hat = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_hat)\n",
    "    \n",
    "    # gradients, aka the backward pass, calculates dl/dw\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights, and since we don't want this update to be part of computation graph, we do the 'with no grad'\n",
    "#     with torch.no_grad():\n",
    "#         w -= lr * w.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # and since we are using auto grad, remember to zero out the grad, otherwise pytorch accumulate the grad values\n",
    "#     w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loggging\n",
    "    if epoch % 2 == 0:\n",
    "        [w, bias] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prediction after training: f(5) = 9.783'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Prediction after training: f(5) = {model(X_test).item():.3f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, if you compare this file with the maunal gradient descent using numpy, this version took 20 epochs to get worse performance than 10 epochs in numpy. This is (?) because the auto grad is not as accurate as numerical calucaltion (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
