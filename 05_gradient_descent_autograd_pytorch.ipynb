{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do linear regression again with pytorch and see how autograd helps us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "# Remember to set requires_grad to True, since we want to use gradient descent with respect to this variable.\n",
    "w = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_hat):\n",
    "    return ((y - y_hat) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables, for now\n",
    "lr = 0.01\n",
    "n_iters = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prediction before training: f(5) = 0.000'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Prediction before training: f(5) = {forward(5):.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    # predict\n",
    "    y_hat = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_hat)\n",
    "    \n",
    "    # gradients, aka the backward pass, calculates dl/dw\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights, and since we don't want this update to be part of computation graph, we do the 'with no grad'\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "    \n",
    "    # and since we are using auto grad, remember to zero out the grad, otherwise pytorch accumulate the grad values\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    # loggging\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prediction after training: f(5) = 9.612'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Prediction after training: f(5) = {forward(5):.3f}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, if you compare this file with the maunal gradient descent using numpy, this version took 20 epochs to get worse performance than 10 epochs in numpy. This is (?) because the auto grad is not as accurate as numerical calucaltion (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
